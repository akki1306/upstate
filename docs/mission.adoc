= Scalable Task Processing Using a Shared Work Queue

:toc:

[options="header"]
|=== 
| ID | Short Name
| `200` | `messaging-work-queue`
|===

== Description

This mission demonstrates how to dispatch tasks to a scalable set of
worker processes using a message queue.  It uses the AMQP 1.0 message
protocol to send and receive messages.  The workers are implemented in
a variety of different languages and application runtimes.

Having a single, well known shared resource for adding and consuming
work makes it easier to scale the number of workers up and down.  The
frontend process does not need to know any of the details of the
backend workers.  The use of a standard messaging protocol enables
frontends and workers written in any language to interoperate.

The mission covers:

* Sending work requests to a common work queue
* Receiving and processing work requests as they come in
* Sending the processed result back to the requesting process
* Adding new workers as demand increases or removing workers as it dies down
* Monitoring the status of the worker processes

== User Problem

The user has a large number of discrete tasks that require processing.
To make sure they are processed with satisfactory speed, the user
wants to be able to add worker processes on demand as the number of
outstanding tasks grows.

One could achieve this by telling every requesting process about every
worker process, but that requires updating many potential requesters.
Instead, the user wants to design the system so that the requester
does not need to know how many workers there are, and the workers do
not need to know which process the request came from.

== Concepts and Architectural Patterns

This mission demonstrates a distributed messaging application.  It
contains multiple components that communicate using a messaging
server.

The frontend, a web server, registers requests to perform work by
sending a request message to a well known queue.  A backend worker
then fetches the work request from the queue and performs the work.

When the frontend creates the request message, it also establishes a
queue for responses.  It includes the address of this queue in its
request.  After processing the work, the backend worker sends the
result as a response to the pre-established response queue.

* Sending and receiving messages.  The application uses the AMQP
  message protocol to communicate using queues and topics on a message
  server.

* Sending request messages and receiving response messages.  The
  application demonstrates how the request-response messaging pattern
  is implemented in AMQP.

* Periodically sending broadcast updates.  The application
  demonstrates use of the publish-subscribe messaging pattern for
  distributing information to multiple consumers.

* Maintaining connections between services.  The frontend and backend
  components reconnect to the message server if the connection is
  lost.

* Scaling backend workers up and down.  The operator can freely add
  and remove backend workers.

== Prerequisites

* The user has access to an OpenShift instance and is logged in.

* The user has selected a project in which the frontend and backend
  processes will be deployed.

== Use Case

=== Deployment

. Apply the OpenShift templates to your current project.  The
  templates create OpenShift entities that are used in subsequent
  steps.
+
[source, shell]
----
oc apply -f templates/
----

. Create the broker application.  This command configures and deploys
  the broker.
+
[source, shell]
----
oc new-app --template=amq63-basic \
  -p APPLICATION_NAME=work-queue-broker \
  -p IMAGE_STREAM_NAMESPACE=$(oc project -q) \
  -p MQ_PROTOCOL=amqp \
  -p MQ_QUEUES=work-queue/requests,work-queue/responses \
  -p MQ_TOPICS=work-queue/worker-updates \
  -p MQ_USERNAME=work-queue \
  -p MQ_PASSWORD=work-queue
----

. Create the frontend application.  This command builds and deploys
  the frontend web server.
+
Replace `<runtime>` with your chosen runtime.  The options are
`nodejs`, `spring-boot`, `wfswarm`, and `vertx`.
+
[source, shell]
----
oc new-app --template=<runtime>-messaging-work-queue-frontend
----

. Create the worker application.  This command builds and deploys the
  backend worker.
+
[source, shell]
----
oc new-app --template=<runtime>-messaging-work-queue-worker
----

=== Operation

. Navigate to the frontend web server.  Use the URL shown under the
  `<runtime>-messaging-work-queue-frontend` application in the project
  overview.

. Enter text in the *Requests* form.

. See the response text in uppercase under *Responses*.

. In the OpenShift console, increase the number of worker pods to two.

. In the frontend web UI, note the presence of two workers under
  *Workers*.

. Submit multiple requests.

. See responses returned from both workers.

== Acceptance Criteria

An HTTP request to `http://<frontend>/api/send-request` with the JSON
payload `{"text": "abc"}` succeeds.  Subsequent requests to
`http://<frontend>/api/data` contain the JSON
`{"responses":[{"workerId":"<id>","text": "ABC"}]}`.

=== Vert.x-specific Acceptance Criteria

=== Swarm-specific Acceptance Criteria

=== Spring Boot-specific Acceptance Criteria

=== Node.js Acceptance Criteria

== Integration Requirements

== Tags

== Notes

Demo code - <https://github.com/ssorj/upstate>

== Approval

|=======
|PM|https://github.com/<username>[Name]|&#x2611;
|DevExp|https://github.com/<username>[Name]|&#x2611;
|Vert.x|https://github.com/<usernname>[Name]|&#x2611;
|WildFly Swarm|https://github.com/<username>[Name]|&#x2611;
|Spring Boot|https://github.com/<username>[Name]|&#x2611;
|Node.js|https://github.com/<username>[Name]|&#x2611;
|QE|https://github.com/<username>[Name]|&#x2611;
|Docs|https://github.com/<username>[Name]|&#x2611;
|Architect|https://github.com/<username>[Name]|&#x2610;
|=======

// unchecked = &#x2610;
// checked = &#x2611;
